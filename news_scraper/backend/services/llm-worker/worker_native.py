"""
LLM Summarization Worker - MongoDB Direct Version
- Polls MongoDB for new enriched articles
- Uses internal asyncio.Queue for buffering
- Runs parallel LLM workers for high throughput
- Handles Rate Limiting
"""

import asyncio
import json
import logging
import os
import time
from datetime import datetime
from typing import Dict, Any

import google.generativeai as genai
from pymongo import MongoClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===========================
# Configuration
# ===========================

GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
WORKER_ID = os.getenv('WORKER_ID', 'worker-1')

# Concurrency & Limits
# Adjust based on your Gemini Tier (Free: 15 RPM, Paid: much higher)
RATE_LIMIT_RPM = int(os.getenv('RATE_LIMIT_RPM', '60')) 
MAX_CONCURRENT_REQUESTS = int(os.getenv('MAX_CONCURRENT_REQUESTS', '10'))
POLL_INTERVAL = int(os.getenv('POLL_INTERVAL', '5'))  # seconds

GEMINI_MODEL = "gemini-2.0-flash-lite"
MONGODB_URI = os.getenv('MONGODB_URI', 'mongodb://localhost:27017/')
MONGODB_DB = os.getenv('MONGODB_DB', 'news_db')
MONGODB_COLLECTION_SUMMARIES = 'summarize'
MONGODB_COLLECTION_ENRICHED = 'enriched_articles'

# ===========================
# Gemini Summarizer
# ===========================

class GeminiSummarizer:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.model = None
        self._initialize()
    
    def _initialize(self):
        try:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel(GEMINI_MODEL)
            logger.info(f"Gemini API initialized: {GEMINI_MODEL}")
        except Exception as e:
            logger.error(f"Failed to initialize Gemini API: {e}")
            raise
    
    async def summarize(self, article: Dict[str, Any]) -> Dict[str, Any]:
        try:
            prompt = self._create_prompt(article)
            
            # Use native async method
            response = await self.model.generate_content_async(prompt)
            
            if not response or not response.text:
                return self._create_fallback_summary(article)
            
            return self._parse_response(response.text, article)
        
        except Exception as e:
            logger.error(f"Error in Gemini summarization: {e}")
            return self._create_fallback_summary(article)

    def _create_prompt(self, article: Dict[str, Any]) -> str:
        content = article.get('content', '')
        content_quality = article.get('content_quality', 'unknown')
        
        # For poor content quality, warn the LLM
        quality_warning = ""
        if content_quality == 'poor' or len(content) < 200:
            quality_warning = "\n**WARNING:** Content fetch may have failed - analyze title carefully."
        
        return f"""You are a strict financial analyst AI. First determine if this news is DIRECTLY RELEVANT to {article.get('company', '').upper()} company's business operations, financials, or stock price.{quality_warning}

**Company:** {article.get('company', '').upper()}
**Title:** {article.get('title', '')}
**Content:** {content[:4000]}
**Factor Type:** {article.get('factor_type', 'unknown')}

**RELEVANCE CRITERIA:**
- Article MUST be primarily about {article.get('company', '').upper()}
- NOT just a brief mention or peripheral reference
- NOT generic industry news unless {article.get('company', '').upper()} is also impacted
- NOT spam, press releases, or promotional content

**Required Output (JSON only):**
{{
    "is_relevant": true/false,
    "relevance_reason": "specific reason why relevant/irrelevant",
    "summary": "2-3 sentence summary (or 'Not relevant to {article.get('company', '')}' if irrelevant)",
    "key_points": ["Point 1", "Point 2"] or [],
    "financial_metrics": {{
        "revenue_impact": "positive/negative/neutral/unknown",
        "stock_price_impact": "bullish/bearish/neutral/unknown",
        "confidence": "high/medium/low"
    }},
    "impact_assessment": "1 sentence on market impact (or 'Not relevant' if irrelevant)"
}}

Be STRICT. If unsure, mark is_relevant as false. Respond with JSON only."""

    def _parse_response(self, response_text: str, article: Dict[str, Any]) -> Dict[str, Any]:
        try:
            # Clean markdown code blocks if present
            clean_text = response_text.replace('```json', '').replace('```', '').strip()
            result = json.loads(clean_text)
            return result
        except Exception:
            return self._create_fallback_summary(article)

    def _create_fallback_summary(self, article: Dict[str, Any]) -> Dict[str, Any]:
        return {
            "summary": "Processing failed",
            "key_points": [],
            "financial_metrics": {},
            "impact_assessment": "Error"
        }

# ===========================
# Async Worker Pool
# ===========================

class LLMWorker:
    def __init__(self):
        self.summarizer = None
        self.mongo_client = None
        self.summaries_collection = None
        self.enriched_collection = None
        
        # Internal Queue for parallel processing
        self.queue = asyncio.Queue(maxsize=100)
        self.active_workers = []
        
        # Rate Limiting
        self.rate_limiter = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
        self.request_interval = 60.0 / RATE_LIMIT_RPM  # Time between requests

    async def initialize(self):
        self.summarizer = GeminiSummarizer(GEMINI_API_KEY)
        
        # MongoDB
        self.mongo_client = await asyncio.to_thread(MongoClient, MONGODB_URI)
        db = self.mongo_client[MONGODB_DB]
        self.summaries_collection = db[MONGODB_COLLECTION_SUMMARIES]
        self.enriched_collection = db[MONGODB_COLLECTION_ENRICHED]
        
        await asyncio.to_thread(
            self.summaries_collection.create_index, 'article_id', unique=True
        )
        
        logger.info(f"Initialized. Workers: {MAX_CONCURRENT_REQUESTS}, RPM Limit: {RATE_LIMIT_RPM}")

    async def processing_worker(self, worker_id: int):
        """Continuous worker loop that pulls from queue"""
        logger.info(f"Worker {worker_id} started")
        
        while True:
            # Get message from queue
            article = await self.queue.get()
            
            try:
                article_id = article.get('article_id')
                
                # Skip articles with extremely poor content (likely scraping failures)
                content = article.get('content', '')
                if len(content) < 100:
                    logger.warning(f"SKIPPING - content too short ({len(content)} chars): {article.get('company')} - {article.get('title')[:40]}...")
                    # Mark as summarized anyway to avoid reprocessing
                    self.enriched_collection.update_one(
                        {'article_id': article_id},
                        {'$set': {'summarized': True, 'summarized_at': datetime.utcnow()}}
                    )
                    continue
                
                # Rate Limiting Logic
                async with self.rate_limiter:
                    # Enforce minimum interval to respect RPM
                    start_time = time.time()
                    
                    # 1. Summarize (The Heavy Lifting)
                    logger.info(f"Worker {worker_id} processing: {article.get('title')[:30]}...")
                    llm_summary = await self.summarizer.summarize(article)
                    
                    # 2. Check relevance - skip irrelevant articles
                    is_relevant = llm_summary.get('is_relevant', True)
                    relevance_reason = llm_summary.get('relevance_reason', 'No reason provided')
                    
                    if not is_relevant:
                        elapsed = time.time() - start_time
                        logger.warning(f"IRRELEVANT (Worker {worker_id}): {article.get('company')} - {article.get('title')[:40]}... | Reason: {relevance_reason} ({elapsed:.1f}s)")
                        # Mark as summarized to avoid reprocessing
                        self.enriched_collection.update_one(
                            {'article_id': article_id},
                            {'$set': {'summarized': True, 'summarized_at': datetime.utcnow()}}
                        )
                        continue
                    
                    # 3. Prepare Result (only for relevant articles)
                    summarized = {
                        '_id': article_id,
                        **{k: v for k, v in article.items() if k != '_id'},
                        **llm_summary,
                        'summarized_at': datetime.utcnow().isoformat(),
                        'worker_id': f"{WORKER_ID}_{worker_id}"
                    }
                    
                    # 4. DB Write (Async wrapper)
                    await asyncio.to_thread(
                        self.summaries_collection.update_one,
                        {'_id': article_id},
                        {'$set': summarized},
                        upsert=True
                    )
                    
                    # 5. Mark enriched article as summarized
                    self.enriched_collection.update_one(
                        {'article_id': article_id},
                        {'$set': {'summarized': True, 'summarized_at': datetime.utcnow()}}
                    )
                    
                    elapsed = time.time() - start_time
                    logger.info(f"SUMMARIZED (Worker {worker_id}): {article.get('company')} - {article.get('title')[:40]}... ({elapsed:.1f}s)")
                    
                    # Enforce RPM wait if needed
                    if elapsed < self.request_interval:
                        await asyncio.sleep(self.request_interval - elapsed)

            except Exception as e:
                logger.error(f"Worker {worker_id} error: {e}")
            finally:
                self.queue.task_done()

    async def run(self):
        """Main loop: Polls MongoDB for new enriched articles"""
        
        # Start the worker pool
        self.active_workers = [
            asyncio.create_task(self.processing_worker(i)) 
            for i in range(MAX_CONCURRENT_REQUESTS)
        ]
        
        try:
            while True:
                # Find unsummarized enriched articles
                unsummarized = list(self.enriched_collection.find({
                    'summarized': {'$ne': True}
                }).limit(50))
                
                if not unsummarized:
                    logger.debug(f"No unsummarized articles, sleeping {POLL_INTERVAL}s...")
                    await asyncio.sleep(POLL_INTERVAL)
                    continue
                
                logger.info(f"Found {len(unsummarized)} articles to summarize")
                
                # Add to queue
                for article in unsummarized:
                    await self.queue.put(article)
                
                # Wait for queue to be processed
                await self.queue.join()
                
        except Exception as e:
            logger.error(f"Main loop error: {e}")
            raise

    async def shutdown(self):
        # Cancel workers
        for task in self.active_workers:
            task.cancel()
        
        if self.mongo_client: 
            self.mongo_client.close()
        logger.info("Shutdown complete")

async def main():
    logger.info("=" * 60)
    logger.info("LLM Summarization Worker - Starting (MongoDB Direct)")
    logger.info("=" * 60)
    
    worker = LLMWorker()
    try:
        await worker.initialize()
        await worker.run()
    except KeyboardInterrupt:
        pass
    finally:
        await worker.shutdown()

if __name__ == "__main__":
    asyncio.run(main())